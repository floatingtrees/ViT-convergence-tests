# ViT-convergence-tests

CNNs are known to learn features faster than VITs, which has primarily been attributed to their inductive spatial bais. The goal of this repository is to find out if we can speed up convergence for a VIT by adding an attention mask in early layers, which only allows the model to attend to patches by each other. As the model trains, the opacity of the mask is scaled down to 0, so this method has no additional cost at inference time. Preliminary experiments suggest a mild performance improvement, but it's difficult to tell from just a few runs. Out of two runs, the masked VIT converges slightly faster in one, with around 2% higher accuracy (45 to 43), but in the second run, with weight decay, they achive very similar results. This experiment is hogging my GPU resources, so I'll stop for now. Feel free to use this repository to check things out.